{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝이란?\n",
    "\n",
    "## 텍스트 마이닝의 정의\n",
    "텍스트로부터 고수준의 정보를 꺼내오는 과정.\n",
    "통계적인 패턴학습으로부터 패턴과 트렌드를 찾아낸다. \n",
    "\n",
    "## 텍스트 마이닝의 목적\n",
    "텍스트라는 비정형 데이터를 정형데이터로 전환하는 것.\n",
    "- 비정형 데이터를 일정한 길이의 vector로 변환한 뒤 변환된 벡터에 머신러닝 기법을 적용시킨다.\n",
    "\n",
    "## 텍스트 마이닝의 이해를 위한 기본요구지식\n",
    "- 자연어처리\n",
    "- 통계학 & 선형대수\n",
    "- 머신러닝\n",
    "- 딥러닝\n",
    "\n",
    "## 텍스트 마이닝 적용분야\n",
    "- Document Classification\n",
    "\t- Sentiment analysis, Classification\n",
    "- Document Generation\n",
    "\t- Q&A, summarization, translation (3가지가 각각 비중이 커져 현재는 단일의 분야로 취급받고 있음)\n",
    "- Keyword extraction\n",
    "\t- tagging/annotation : NER\n",
    "- Topic modeling\n",
    "\t- LSA, LDA \n",
    "\n",
    "## 텍스트 마이닝 도구 - 파이썬 (강력한 도구이며 R에서 파이썬으로 흐름이 옮겨가는 추세)\n",
    "- NLTK : 가장 많이 알려진 NLP 라이브러리\n",
    "- Scikit Learn : 머신러닝 라이브러리, 기본적인 NLP를 지원하며 NLTK와 함께사용할수도있음\n",
    "- Gensim : Word2Vec으로 유명해졌으나 최근에는 자주 사용되지 않음.\n",
    "- Keras : RNN, seq2seq등 딥러닝 라이브러리 제공\n",
    "- PyTorch : 딥러닝에서 최근 뜨거운 관심을 받고있음.\n",
    "\n",
    "##NLP\n",
    "- 목적 Document, sentence 등을 sparse vector로 변환(머신러닝에 쓸수있는 벡터로 변환하기 위함)\n",
    "\n",
    "- Tokenize : 문서,문장을 최소단위로 쪼갠다.\n",
    "문서를 문장으로 분리, 문장을 단어의 집합으로 분리, 의미없는 문자등을 걸러낸다. 영어는 공백기준으로 비교적 쉽게 Tokenize할 수 있다. 하지만 한글은 구조상 어렵다.\n",
    "\n",
    "- Text normalization : 최소 단위를 표준화한다.\n",
    "동일한 의미의 단어가 다른 형태를 갖는 것을 보완한다.\n",
    "어간추출과, 표제어 추출등을 사용한다.\n",
    "문제점 : 사전에 없는 이상한 단어로 변환되기도 한다. 이는 알고리즘에 의해 변환되기 때문인데, 실제로는 큰 문제가 없다.\n",
    "\n",
    "- POS-tagging : 최소 의미단위로 나누어진 대상에 품사를 부착한다.\n",
    "형태소 분석이라고 번역되기도 하는데 이는 정확한 번역이 아니다.\n",
    "한글 형태소 분석기 도구등을 통해 체험해보는 것이 가능하다.\n",
    "\n",
    "- Chunking : POS-tagging의 결과를 명사구,형용사구,분사구와 같은 말모듬으로 다시 합친다.\n",
    "서로 겹치지 않으면서 의미가 있는 구로 묶어나가는 과정이다.\n",
    "NER(개체명 인식) : 기관,단체,사람,날짜 등의 특정정보에 해당하는 명사구\n",
    "NER은 텍스트로부터 뭔가 의미있는 정보를 추출하기 위한 방법으로 사용\n",
    "챗봇에서 자주 사용된다.\n",
    "\n",
    "- BOW,TFIDF : Tokenized 결과를 이용해 문서를 Vector로 표현한다.\n",
    "BOW(Bag of Words) : 순서를 배제하고 분류한다. \n",
    "모든 문서에 한번이상 나타난 단어들에 대해 유(1)/무(0)로 문서를 표현한다.\n",
    "\n",
    "TFIDF(Term Frequency - Inverse Document Frequency) : 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올린다.\n",
    "\n",
    "Naive Bayes : 스팸이냐 아니냐를 분석하는 간단한 분류도구로 시작 됨. 예전엔 준수한 정확도를 자랑했으나 지금은 그렇지 않다.\n",
    "\n",
    "Logistic Regression : 로지스틱 회귀분석을 텍스트마이닝에 사용하려고 하면 부적합하다. 과적합이 발생하기 쉽고, 많은 데이터셋이 필요하기 때문이다. 그럼에도 불구하고 적은 데이터셋에서도 잘 작동하긴 한다.\n",
    "\n",
    "## 텍스트 마이닝의 문제점\n",
    "\n",
    "### 차원의 저주\n",
    "- 데이터가 너무 적어 각 데이터 간의 거리가 너무 멀게 위치하는 단점이 생긴다.\n",
    "- 해결방법으로는 더 많은 데이터를 갖는 것과 차원을 축소하는 방법이 있다.\n",
    "\n",
    "### 단어 빈도의 불균형\n",
    "- Zipf's law(멱법칙) : 극히 소수의 데이터가 결정적인 영향을 미치게 됨\n",
    "- 해결방안으로는 빈도 높은 단어를 삭제, Boolean BOW사용, log등의 함수를 사용한 weight 조정의 방법이 있음.\n",
    "\n",
    "### 단어가 쓰인 순서정보의 손실\n",
    "\n",
    "### 문제의 해결방안\n",
    "- 차원 축소\n",
    "- Feature Extraction \n",
    "PCA 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 변환해 차원을 축소한다.\n",
    "- 주성분 분석\n",
    "\n",
    "## Word Embedding\n",
    "단어를 어떻게 컴퓨터안에서 표현하고 담을것인가에 대한 문제에서 고안\n",
    "One-hot-encoding : 하나만 hot하다는 뜻. 순서가 생기면 안되므로 의도하지 않은 거리관계가 나오지 않도록 함.\n",
    "one-hot-encoding으로 표현된 단어를 dense vector로 변환\n",
    "변환된 vector를 이용해 학습\n",
    "최종목적에 맞게 학습을 위해 변환함.\n",
    "\n",
    "BOW와는 다른 관점의 문서 표현\n",
    "document : 제한된 maxlen개의 word sequence\n",
    "word : one-hot-vector에서 저차원으로 embedding된 dense vector\n",
    "즉 document는 (maxlen,reduced_dim)의 2차원 행렬로 표현\n",
    "단순한 분류모형(sequence 무시)을 사용하거나 딥러닝을 활용한다.\n",
    "\n",
    "### Word2Vec\n",
    "단어의 위치에 기반해 의미를 내포하는 vector 생성\n",
    "비슷한 위치에 나타나는 단어들의 유사성을 이용해 연산을 가능케함.\n",
    "\n",
    "### ELMo\n",
    "- Word2Vec과 기본적인 개념은 비슷하다.\n",
    "- 둘 다 임베딩이 되었을 때 주변에 비슷한 값을 갖도록 하는 목적을 지님.\n",
    "- ELMo는 딥러닝에 대한 지식이 부족하다면 사용하기에 어려움을 겪을 수 있다.\n",
    "- 정방향의 문맥과 역방향의 문맥을 전부 사용한다. \n",
    "\n",
    "### Document Embedding\n",
    "- Word2Vec 모형을 갖고 있으며 document의 고유한 vector를 함께 학습함으로써 document에 대한 dense vector를 생성한다.\n",
    "- 이 dense vector를 이용해 매칭, 분류 등의 작업을 수행\n",
    "- 처음엔 각광받았으나 지금은 관심받지못하고 있다.\n",
    "\n",
    "- N-gram : 문맥을 파악하기 위한 전통적 방법\n",
    "대상이 되는 문자열을 하나의단어 단위가 아닌, 두개 이상의 단위로 잘라서 처리한다.\n",
    "\n",
    "### sequence to sequence : 번역, chat-bot을 위해 고안됨.\n",
    "- encoder와 decoder의 구조를 가짐.\n",
    "->>>>>>\n",
    "### Attention : 출력에 나온 어떤 단어가 입력에 있는 특정 단어들에 민감한 것에서 착안됨. \n",
    "- 입력 단어로부터 출력 단어에 직접 링크를 만듬.\n",
    "->>>>>>\n",
    "### Transformer : 입력 단어들끼리도 상호연관성이 있는 것에 착안\n",
    "- 입력 -> 출력으로의 attention 외에 입력 단어들 간의 attention추가\n",
    "- encoder와 decoder가 서로 다른 attention 구조를 사용 RNN,이 사라지고 self-attention이 그 자리를 차지\n",
    "### BERT\n",
    "- 양방향 transformer 인코더를 사용\n",
    "- 거의 모든 분야에서 top score를 기록"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
